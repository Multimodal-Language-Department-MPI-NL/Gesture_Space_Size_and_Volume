# Gesture_Space_Size_and_Volume

> Attribution: Use the script prepared for the focus group session. Attribute to this Envision Box module: [Module](https://www.envisionbox.org/embedded_MergingMultimodal_inPython.html)

Quantitative measures for gesture space analysis using MediaPipe keypoints, including vertical amplitude, McNeillian space classification, and volumetric space calculations.

## ğŸ”¬ Research Context

This module shows how to calculate maximum vertical amplitude (i.e., gesture height), characterize the location of gestures based on McNeillian space (McNeill, 1992), and calculate the 2D size or 3D volume of gesture (or sign) space. 

## ğŸ¯ What This Project Does

1. **Vertical Amplitude Analysis**: Calculate maximum hand height relative to body landmarks
2. **McNeillian Space Classification**: Map hand positions to gesture space zones (center, periphery, extra-periphery)
3. **Volumetric Space Calculation**: Measure the 3D space volume utilized by gestures
4. **Multi-file Processing**: Batch analysis across multiple participants and gesture segments

## ğŸ“Š Analysis Process

1. **Load Data**: Import MediaPipe keypoint time series
2. **Convert Format**: Transform MediaPipe landmarks to OpenPose-like structure
3. **Calculate Vertical Amplitude**: Determine hand height relative to body reference points
4. **Map McNeillian Space**: Classify hand positions into gesture space zones
5. **Compute Volumetric Space**: Calculate 3D volume/2D area utilized by gestures

## ğŸ”§ Methods

### Vertical Amplitude
- **Body-relative scaling**: Height measured relative to body landmarks (midline, shoulders, face)
- **6-level classification**: 0=below midline, 1=midline to middle-upper body, 2=above middle-upper body, 3=between shoulders and face middle, 4=between face middle and head top, 5=above head
- **Hand selection**: Support for single hand (L/R) or both hands (B)

### McNeillian Space
- **Grid definition**: Dynamic grid based on body landmarks (hips, shoulders, eyes)
- **Zone classification**: Center-center, Center, Periphery, Extra-periphery
- **Subsection mapping**: 8 directional subsections for periphery zones
- **Metrics**: Space usage count, maximum peripheral space, modal space usage

### Volumetric Space
- **3D volume calculation**: X, Y, Z axis ranges for 3D data
- **2D area calculation**: X, Y axis ranges for 2D data
- **Dynamic expansion**: Bounding box grows with gesture movement
- **Hand-specific analysis**: Individual or combined hand volumes

## ğŸ“ Project Structure

```
Macneillian_Space_and_2D_Size/
â”œâ”€â”€ README.md
â”œâ”€â”€ environment.yml
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01-analysis_kinematic_features_module.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/                     # MediaPipe keypoint CSVs
â”‚   â””â”€â”€ raw/                          # Raw input files
â”œâ”€â”€ gesture_space.jpg                 # McNeillian space diagram
â””â”€â”€ results/
    â””â”€â”€ space_results.xlsx           # Analysis outputs
```

## ğŸš€ Quick Start

### Prerequisites

```bash
conda env create -f environment.yml
conda activate macneillian
```

### Run the Interactive Notebook

```bash
jupyter lab
# Open notebooks/01-analysis_kinematic_features_module.ipynb
```


## ğŸ“ˆ Data Format

### Input
- **MediaPipe CSVs**: Time series with keypoint coordinates (X, Y, Z) and timestamps
- **ELAN files**: Optional annotation files for gesture segmentation

### Output
- **Excel files**: Per-gesture metrics including space usage, volumes, and classifications
- **Analysis results**: Vertical amplitude, McNeillian space metrics, volumetric measures

## ğŸ›ï¸ Configuration

### Analysis Parameters
- **Hand selection**: 'L' (left), 'R' (right), or 'B' (both)
- **Data dimensionality**: 2D or 3D analysis
- **Gesture segmentation**: Optional ELAN annotation integration

### McNeillian Space Parameters
- **Grid scaling**: Based on body width and face measurements
- **Zone boundaries**: Dynamic calculation from body landmarks
- **Subsection mapping**: 8-directional classification system

## ğŸ”— Related Projects

- [Extracting MediaPipe keypoints](https://github.com/Multimodal-Language-Department-MPI-NL/MediaPipe_keypoints_extraction): Extract pose landmarks from video with MediaPipe.
- [Smoothing](https://github.com/Multimodal-Language-Department-MPI-NL/Smoothing): Smooth noise and interpolate missing points.
- [Normalization](https://github.com/Multimodal-Language-Department-MPI-NL/Normalization): Normalize position and scale across files.
- [Speed, Acceleration, and Jerk](https://github.com/Multimodal-Language-Department-MPI-NL/Speed_Acceleration_Jerk): Compute speed, acceleration, and jerk over time.
- [Merging ELAN and MediaPipe](https://github.com/Multimodal-Language-Department-MPI-NL/Merging_Motion_ELAN): Align motion data with ELAN annotations.

## ğŸ“– References

- McNeill, D. (1992). *Hand and Mind: What Gestures Reveal About Thought*
- Pouw, W. (2023). Analysis kinematic features module. [https://envisionbox.org/embedded_Analysis_kinematic_features_module.html](https://envisionbox.org/embedded_Analysis_kinematic_features_module.html)

## ğŸ¤ Contributing

This project is part of the MPI Multimodal Interaction Research framework. For questions or contributions, please refer to the main project documentation.

## ğŸ“„ License

This project is part of the MPI research framework. Please refer to the main project license for usage terms.
